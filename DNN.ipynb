{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Environment & TPU setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Colab Pro → TPU v2-8\n",
    "%tensorflow_version 2.x\n",
    "import os, json, math, datetime as dt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Mixed-precision (bfloat16) for TPU\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_bfloat16')\n",
    "\n",
    "# One-time installs\n",
    "!pip install polars==0.20.19 gcsfs==2024.4.1 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Data access (GCS → Polars DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "BUCKET = \"nyc-taxi-fhv-460946772036\"\n",
    "!gcsfuse --implicit-dirs $BUCKET /mnt/fhv\n",
    "\n",
    "PARQUET = \"/mnt/fhv/fhvhv_all_years.zstd.parquet\"\n",
    "import polars as pl\n",
    "df = pl.read_parquet(PARQUET, low_memory=False)\n",
    "print(df.shape)          # 745 M × 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Cleaning & target creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Clip extreme miles / time\n",
    "CLIP = {\"trip_miles\": (0.1, 200),\n",
    "        \"trip_time\":  (60, 4*3600)}\n",
    "for c,(lo,hi) in CLIP.items():\n",
    "    df = df.with_columns(\n",
    "        pl.when(pl.col(c).is_between(lo,hi))\n",
    "          .then(pl.col(c)).otherwise(None)\n",
    "    )\n",
    "\n",
    "money = [\"base_passenger_fare\",\"tolls\",\"bcf\",\n",
    "         \"sales_tax\",\"congestion_surcharge\",\"airport_fee\"]\n",
    "df = df.with_columns([pl.col(c).clip(0) for c in money])\n",
    "\n",
    "df = df.with_columns([\n",
    "    ( sum(pl.col(c) for c in money) ).alias(\"target_amount\"),\n",
    "    (pl.col(\"trip_miles\") / (pl.col(\"trip_time\")/3600)).alias(\"mph\")\n",
    "]).drop_nulls(\"target_amount\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "df = df.with_columns([\n",
    "    pl.col(\"pickup_datetime\").dt.hour().alias(\"pickup_hour\"),\n",
    "    pl.col(\"pickup_datetime\").dt.weekday().alias(\"pickup_wday\"),\n",
    "    pl.col(\"pickup_datetime\").dt.month().alias(\"pickup_month\"),\n",
    "])\n",
    "\n",
    "high_card = [\"dispatching_base_num\",\"PULocationID\",\"DOLocationID\",\n",
    "             \"hvfhs_license_num\"]\n",
    "for col in high_card:\n",
    "    df = df.with_columns(pl.col(col).fill_null(\"UNK\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Train/valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "cutoff = dt.datetime(2022,12,1)\n",
    "train_df = df.filter(pl.col(\"pickup_datetime\") <  cutoff)\n",
    "valid_df = df.filter(pl.col(\"pickup_datetime\") >= cutoff)\n",
    "print(train_df.shape, valid_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Write TFRecord shards (optional)\n",
    "\n",
    "> Skip this section if you have plenty of RAM and prefer a direct in-RAM `tf.data.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import tensorflow as tf, itertools, tqdm\n",
    "\n",
    "def df_to_tfr_iter(tbl, batch=200_000):\n",
    "    for i in range(0, tbl.height, batch):\n",
    "        chunk = tbl.slice(i, batch)\n",
    "        yield dict(chunk.to_arrow().to_pydict())\n",
    "\n",
    "def write_tfr(split, tbl):\n",
    "    outdir = f\"/content/tfr/{split}\"\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    for shard_id, recs in enumerate(df_to_tfr_iter(tbl)):\n",
    "        fn = f\"{outdir}/{split}-{shard_id:05d}.tfr\"\n",
    "        with tf.io.TFRecordWriter(fn, compression_type=\"GZIP\") as w:\n",
    "            for j in range(len(recs[\"target_amount\"])):\n",
    "                feat = {\n",
    "                    k: (tf.train.Feature(\n",
    "                            float_list=tf.train.FloatList(value=[recs[k][j]]))\n",
    "                         if isinstance(recs[k][j], float)\n",
    "                         else tf.train.Feature(\n",
    "                            bytes_list=tf.train.BytesList(\n",
    "                                value=[str(recs[k][j]).encode()])))\n",
    "                    for k in recs\n",
    "                }\n",
    "                w.write(tf.train.Example(\n",
    "                        features=tf.train.Features(feature=feat)\n",
    "                     ).SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write both splits **once**, expect ≈ 12 min."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 tf.data input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "FEATURES = {\n",
    "    # floats\n",
    "    **{c: tf.io.FixedLenFeature([], tf.float32) for c in\n",
    "       [\"trip_miles\",\"trip_time\",\"mph\",\"base_passenger_fare\",\"tolls\",\n",
    "        \"bcf\",\"sales_tax\",\"congestion_surcharge\",\"airport_fee\"]},\n",
    "    # ints\n",
    "    **{c: tf.io.FixedLenFeature([], tf.int64) for c in\n",
    "       [\"pickup_hour\",\"pickup_wday\",\"pickup_month\"]},\n",
    "    # strings (to be hashed)\n",
    "    **{c: tf.io.FixedLenFeature([], tf.string) for c in\n",
    "       [\"hvfhs_license_num\",\"dispatching_base_num\",\n",
    "        \"PULocationID\",\"DOLocationID\",\n",
    "        \"shared_request_flag\",\"shared_match_flag\",\n",
    "        \"wav_request_flag\",\"access_a_ride_flag\"]},\n",
    "    \"target_amount\": tf.io.FixedLenFeature([], tf.float32),\n",
    "}\n",
    "\n",
    "def parse(ex):\n",
    "    return tf.io.parse_single_example(ex, FEATURES)\n",
    "\n",
    "def ds_from_tfr(split, batch, shuffle=False):\n",
    "    files = tf.io.gfile.glob(f\"/content/tfr/{split}/*.tfr\")\n",
    "    ds = tf.data.TFRecordDataset(files, compression_type=\"GZIP\",\n",
    "                                 num_parallel_reads=tf.data.AUTOTUNE)\n",
    "    ds = ds.map(parse, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if shuffle: ds = ds.shuffle(1_000_000)\n",
    "    return (ds.batch(batch, drop_remainder=True)\n",
    "             .prefetch(tf.data.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Deep & Cross Network v2  (6-layer ReLU deep tower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from tensorflow.keras import layers as L, regularizers, Model\n",
    "\n",
    "HASH_BUCKETS = 2000\n",
    "EMB_DIM      = 8\n",
    "L2_REG       = 1e-5\n",
    "\n",
    "inputs = {k: L.Input(shape=(), name=k,\n",
    "                     dtype=tf.string if FEATURES[k].dtype==tf.string else tf.float32)\n",
    "          for k in FEATURES if k != \"target_amount\"}\n",
    "\n",
    "def hash_embed(feat):\n",
    "    idx = tf.strings.to_hash_bucket_fast(inputs[feat], HASH_BUCKETS)\n",
    "    emb = L.Embedding(HASH_BUCKETS, EMB_DIM)(idx)\n",
    "    return L.Flatten()(emb)\n",
    "\n",
    "embed_out = [hash_embed(f) for f in\n",
    "             [\"hvfhs_license_num\",\"dispatching_base_num\",\n",
    "              \"PULocationID\",\"DOLocationID\"]]\n",
    "\n",
    "# 0/1 flags\n",
    "flags = [L.Cast(dtype='float32')(inputs[f])\n",
    "         for f in [\"shared_request_flag\",\"shared_match_flag\",\n",
    "                   \"wav_request_flag\",\"access_a_ride_flag\"]]\n",
    "\n",
    "# Numeric norm\n",
    "num_cols = [\"trip_miles\",\"trip_time\",\"mph\",\"base_passenger_fare\",\n",
    "            \"tolls\",\"bcf\",\"sales_tax\",\"congestion_surcharge\",\"airport_fee\"]\n",
    "norm = L.Normalization()\n",
    "norm.adapt(train_df.select(num_cols).to_numpy())  # offline\n",
    "num_out = norm(L.Concatenate()([inputs[c] for c in num_cols]))\n",
    "\n",
    "# Cyclic time\n",
    "hour = tf.cast(inputs[\"pickup_hour\"], tf.float32)\n",
    "sin_hour = tf.sin(2*tf.constant(math.pi)*hour/24)\n",
    "cos_hour = tf.cos(2*tf.constant(math.pi)*hour/24)\n",
    "\n",
    "concat = L.Concatenate()(embed_out + flags + [num_out, sin_hour, cos_hour])\n",
    "\n",
    "# --- Cross stack (3 layers) ---\n",
    "cross = concat\n",
    "for _ in range(3):\n",
    "    cross = tf.keras.experimental.LinearCombination()([concat, cross])\n",
    "\n",
    "# --- Deep tower (6 layers, ReLU) ---\n",
    "x = concat\n",
    "for units, drop in [(1024,0.30),(512,0.30),(256,0.25),\n",
    "                    (128,0.20),(64,0.10),(32,0.10)]:\n",
    "    x = L.Dense(units, activation='relu',\n",
    "                kernel_regularizer=regularizers.l2(L2_REG))(x)\n",
    "    x = L.BatchNormalization()(x)\n",
    "    if drop: x = L.Dropout(drop)(x)\n",
    "\n",
    "fused = L.Concatenate()([cross, x])\n",
    "out   = L.Dense(1, name='fare')(fused)\n",
    "\n",
    "model = Model(inputs, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Train (SGD + Nesterov, MAE loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# LR schedule: linear warm-up 5 epochs → cosine decay to 2 %\n",
    "steps_per_epoch = math.ceil(len(train_df) / 16384)\n",
    "\n",
    "lr_sched = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=0.04,        # scaled for 16 384 batch\n",
    "    decay_steps=15*steps_per_epoch,\n",
    "    alpha=0.02)\n",
    "\n",
    "optim = tf.keras.optimizers.SGD(\n",
    "    learning_rate=lr_sched,\n",
    "    momentum=0.9, nesterov=True)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optim,\n",
    "    loss=tf.keras.losses.MeanAbsoluteError(),\n",
    "    metrics=[tf.keras.metrics.MeanAbsoluteError(name='MAE'),\n",
    "             tf.keras.metrics.MeanAbsolutePercentageError(name='MAPE')],\n",
    "    run_eagerly=False)\n",
    "\n",
    "# Gradient clip\n",
    "@tf.function\n",
    "def train_step(data):\n",
    "    x, y = data\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x, training=True)\n",
    "        loss = model.compiled_loss(y, y_pred)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    grads, _ = tf.clip_by_global_norm(grads, 1.0)\n",
    "    optim.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    model.compiled_metrics.update_state(y, y_pred)\n",
    "    return {m.name: m.result() for m in model.metrics}\n",
    "model.train_step = train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "BATCH = 16_384\n",
    "train_ds = ds_from_tfr('train', BATCH, shuffle=True)\n",
    "valid_ds = ds_from_tfr('valid', BATCH)\n",
    "\n",
    "cb = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_MAE',\n",
    "                                     patience=2, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        'checkpoints/dcnv2_best.keras',\n",
    "        monitor='val_MAE', save_best_only=True),\n",
    "    tf.keras.callbacks.TensorBoard('logs', update_freq=200)   # every 200 steps\n",
    "]\n",
    "\n",
    "history = model.fit(train_ds,\n",
    "                    validation_data=valid_ds,\n",
    "                    epochs=15,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    validation_steps=math.ceil(len(valid_df)/BATCH),\n",
    "                    callbacks=cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Success thresholds**  \n",
    "* `val_MAE ≤ $2.00` (primary)  \n",
    "* `val_MAPE ≤ 12 %`, P95 abs. error ≤ $8  \n",
    "* Train : valid MAE ratio < 1.15\n",
    "\n",
    "Training will stop early if `val_MAE` fails to improve for two consecutive epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 Evaluate & error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np, matplotlib.pyplot as plt\n",
    "pred = model.predict(valid_ds, verbose=0).flatten()\n",
    "truth = valid_df[\"target_amount\"].to_numpy()\n",
    "\n",
    "print(\"final MAE $\", np.mean(np.abs(pred-truth)))\n",
    "p95 = np.percentile(np.abs(pred-truth), 95)\n",
    "print(\"P95 abs. error $\", p95)\n",
    "\n",
    "plt.hist(truth-pred, bins=100)\n",
    "plt.title(\"Residuals ($ actual – predicted)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Save model & wrap-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "model.save(\"dcnv2_sgd_savedmodel\")\n",
    "!zip -r model.zip dcnv2_sgd_savedmodel -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deliver `model.zip` to production (or ML ops pipeline)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}